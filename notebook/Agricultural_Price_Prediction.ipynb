{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ”¬ Research-Grade Agricultural Commodity Price Prediction\n",
                "## Advanced Deep Learning for Multi-Horizon Time Series Forecasting\n",
                "\n",
                "**CSE 3793 - Major Assignment**\n",
                "\n",
                "This notebook implements **7 state-of-the-art deep learning architectures** for agricultural commodity price prediction:\n",
                "\n",
                "1. **Temporal Fusion Transformer (TFT)** - Interpretable multi-horizon forecasting\n",
                "2. **N-BEATS** - Neural Basis Expansion Analysis\n",
                "3. **Informer** - Efficient long-sequence transformer\n",
                "4. **WaveNet** - Dilated causal convolutions\n",
                "5. **DeepAR** - Probabilistic autoregressive RNN\n",
                "6. **TCN** - Temporal Convolutional Network\n",
                "7. **Hybrid Attention Network** - CNN + LSTM + Transformer fusion\n",
                "\n",
                "**Total Parameters: ~20M+**\n",
                "\n",
                "---"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Environment Setup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import numpy as np\n",
                "import pandas as pd\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# TensorFlow/Keras\n",
                "import tensorflow as tf\n",
                "from tensorflow import keras\n",
                "from tensorflow.keras import layers, Model, regularizers\n",
                "from tensorflow.keras.layers import *\n",
                "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
                "from sklearn.preprocessing import MinMaxScaler\n",
                "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
                "\n",
                "# GPU Setup\n",
                "print(f\"TensorFlow: {tf.__version__}\")\n",
                "gpus = tf.config.list_physical_devices('GPU')\n",
                "if gpus:\n",
                "    for gpu in gpus:\n",
                "        tf.config.experimental.set_memory_growth(gpu, True)\n",
                "    tf.keras.mixed_precision.set_global_policy('mixed_float16')\n",
                "    print(f\"GPU: {gpus[0].name}\")\n",
                "    print(\"Mixed Precision: Enabled\")\n",
                "else:\n",
                "    print(\"No GPU detected\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Configuration\n",
                "DATA_PATH = \"/home/draxxy/Downloads/archive/Price_Agriculture_commodities_Week.csv\"\n",
                "SEQUENCE_LENGTH = 30\n",
                "EPOCHS = 200\n",
                "BATCH_SIZE = 32\n",
                "SEED = 42\n",
                "\n",
                "np.random.seed(SEED)\n",
                "tf.random.set_seed(SEED)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 2. Custom Layers for Advanced Architectures"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "@keras.saving.register_keras_serializable()\n",
                "class GatedLinearUnit(layers.Layer):\n",
                "    \"\"\"Gated Linear Unit for Temporal Fusion Transformer.\"\"\"\n",
                "    def __init__(self, units, dropout=0.1, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.units = units\n",
                "        self.dropout_rate = dropout\n",
                "        \n",
                "    def build(self, input_shape):\n",
                "        self.linear = Dense(self.units)\n",
                "        self.gate = Dense(self.units, activation='sigmoid')\n",
                "        self.dropout = Dropout(self.dropout_rate)\n",
                "        self.norm = LayerNormalization()\n",
                "        \n",
                "    def call(self, x, training=None):\n",
                "        linear_out = self.linear(x)\n",
                "        gate_out = self.gate(x)\n",
                "        gated = linear_out * gate_out\n",
                "        gated = self.dropout(gated, training=training)\n",
                "        return self.norm(gated)\n",
                "    \n",
                "    def get_config(self):\n",
                "        return {'units': self.units, 'dropout': self.dropout_rate}\n",
                "\n",
                "\n",
                "@keras.saving.register_keras_serializable()\n",
                "class PositionalEncoding(layers.Layer):\n",
                "    \"\"\"Sinusoidal Positional Encoding.\"\"\"\n",
                "    def __init__(self, max_len=1000, d_model=64, **kwargs):\n",
                "        super().__init__(**kwargs)\n",
                "        self.max_len = max_len\n",
                "        self.d_model = d_model\n",
                "        \n",
                "    def build(self, input_shape):\n",
                "        position = np.arange(self.max_len)[:, np.newaxis]\n",
                "        div_term = np.exp(np.arange(0, self.d_model, 2) * -(np.log(10000.0) / self.d_model))\n",
                "        pe = np.zeros((self.max_len, self.d_model))\n",
                "        pe[:, 0::2] = np.sin(position * div_term)\n",
                "        pe[:, 1::2] = np.cos(position * div_term)\n",
                "        self.pe = tf.constant(pe[np.newaxis, :, :], dtype=tf.float32)\n",
                "        \n",
                "    def call(self, x):\n",
                "        return x + self.pe[:, :tf.shape(x)[1], :]\n",
                "    \n",
                "    def get_config(self):\n",
                "        return {'max_len': self.max_len, 'd_model': self.d_model}"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 3. Data Loading & Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load data\n",
                "df = pd.read_csv(DATA_PATH)\n",
                "print(f\"Dataset: {df.shape[0]:,} records, {df.shape[1]} columns\")\n",
                "print(f\"Columns: {df.columns.tolist()}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def preprocess_data(df):\n",
                "    \"\"\"Comprehensive data preprocessing.\"\"\"\n",
                "    data = df.copy()\n",
                "    data.columns = data.columns.str.lower().str.strip().str.replace(' ', '_')\n",
                "    \n",
                "    # Parse dates\n",
                "    data['date'] = pd.to_datetime(data['arrival_date'], format='%d-%m-%Y', errors='coerce')\n",
                "    data = data.dropna(subset=['date']).sort_values('date')\n",
                "    \n",
                "    # Convert prices\n",
                "    for col in ['min_price', 'max_price', 'modal_price']:\n",
                "        data[col] = pd.to_numeric(data[col], errors='coerce')\n",
                "    \n",
                "    # Remove outliers\n",
                "    Q1, Q99 = data['modal_price'].quantile([0.01, 0.99])\n",
                "    data = data[(data['modal_price'] >= Q1) & (data['modal_price'] <= Q99)]\n",
                "    \n",
                "    return data\n",
                "\n",
                "processed_df = preprocess_data(df)\n",
                "print(f\"Preprocessed: {len(processed_df):,} records\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Aggregate to daily\n",
                "daily_df = processed_df.groupby('date')['modal_price'].mean().reset_index()\n",
                "daily_df = daily_df.sort_values('date')\n",
                "\n",
                "# Interpolate missing dates\n",
                "date_range = pd.date_range(daily_df['date'].min(), daily_df['date'].max(), freq='D')\n",
                "daily_df = daily_df.set_index('date').reindex(date_range).interpolate().reset_index()\n",
                "daily_df.columns = ['date', 'price']\n",
                "\n",
                "print(f\"Daily records: {len(daily_df)}\")\n",
                "print(f\"Date range: {daily_df['date'].min()} to {daily_df['date'].max()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 4. Advanced Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def create_comprehensive_features(df, target='price'):\n",
                "    \"\"\"Create research-grade features.\"\"\"\n",
                "    data = df.copy()\n",
                "    \n",
                "    # Temporal features\n",
                "    data['day_of_week'] = data['date'].dt.dayofweek\n",
                "    data['day_of_month'] = data['date'].dt.day\n",
                "    data['month'] = data['date'].dt.month\n",
                "    data['quarter'] = data['date'].dt.quarter\n",
                "    data['is_month_start'] = data['date'].dt.is_month_start.astype(int)\n",
                "    data['is_month_end'] = data['date'].dt.is_month_end.astype(int)\n",
                "    \n",
                "    # Cyclical encoding\n",
                "    data['day_sin'] = np.sin(2 * np.pi * data['day_of_week'] / 7)\n",
                "    data['day_cos'] = np.cos(2 * np.pi * data['day_of_week'] / 7)\n",
                "    data['month_sin'] = np.sin(2 * np.pi * data['month'] / 12)\n",
                "    data['month_cos'] = np.cos(2 * np.pi * data['month'] / 12)\n",
                "    \n",
                "    # Lag features\n",
                "    for lag in [1, 2, 3, 5, 7, 14, 21, 30]:\n",
                "        if lag < len(data):\n",
                "            data[f'lag_{lag}'] = data[target].shift(lag)\n",
                "            data[f'diff_{lag}'] = data[target].diff(lag)\n",
                "            data[f'pct_change_{lag}'] = data[target].pct_change(lag)\n",
                "    \n",
                "    # Rolling statistics\n",
                "    for window in [3, 5, 7, 14, 21, 30]:\n",
                "        if window < len(data):\n",
                "            data[f'sma_{window}'] = data[target].rolling(window, min_periods=1).mean()\n",
                "            data[f'std_{window}'] = data[target].rolling(window, min_periods=1).std()\n",
                "            data[f'min_{window}'] = data[target].rolling(window, min_periods=1).min()\n",
                "            data[f'max_{window}'] = data[target].rolling(window, min_periods=1).max()\n",
                "            data[f'skew_{window}'] = data[target].rolling(window, min_periods=1).skew()\n",
                "    \n",
                "    # EMA features\n",
                "    for span in [3, 7, 14, 21]:\n",
                "        data[f'ema_{span}'] = data[target].ewm(span=span, adjust=False).mean()\n",
                "    \n",
                "    # Technical indicators\n",
                "    data['rsi'] = compute_rsi(data[target], 14)\n",
                "    data['macd'] = data['ema_7'] - data['ema_21'] if 'ema_7' in data.columns and 'ema_21' in data.columns else 0\n",
                "    \n",
                "    # Bollinger Bands\n",
                "    if 'sma_14' in data.columns and 'std_14' in data.columns:\n",
                "        data['bb_upper'] = data['sma_14'] + 2 * data['std_14']\n",
                "        data['bb_lower'] = data['sma_14'] - 2 * data['std_14']\n",
                "        data['bb_width'] = (data['bb_upper'] - data['bb_lower']) / (data['sma_14'] + 1e-8)\n",
                "    \n",
                "    # Fill NaN\n",
                "    data = data.ffill().bfill().fillna(0)\n",
                "    \n",
                "    return data\n",
                "\n",
                "def compute_rsi(series, period=14):\n",
                "    \"\"\"Compute RSI indicator.\"\"\"\n",
                "    delta = series.diff()\n",
                "    gain = delta.where(delta > 0, 0).rolling(window=period, min_periods=1).mean()\n",
                "    loss = (-delta.where(delta < 0, 0)).rolling(window=period, min_periods=1).mean()\n",
                "    rs = gain / (loss + 1e-8)\n",
                "    return 100 - (100 / (1 + rs))\n",
                "\n",
                "featured_df = create_comprehensive_features(daily_df)\n",
                "print(f\"Total features: {len(featured_df.columns)}\")\n",
                "featured_df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare sequences\n",
                "feature_cols = [c for c in featured_df.columns if c not in ['date']]\n",
                "target_col = 'price'\n",
                "\n",
                "# Scale data\n",
                "scaler = MinMaxScaler()\n",
                "scaled_data = scaler.fit_transform(featured_df[feature_cols])\n",
                "\n",
                "target_scaler = MinMaxScaler()\n",
                "target_scaler.fit(featured_df[[target_col]])\n",
                "\n",
                "# Create sequences\n",
                "def create_sequences(data, seq_length, target_idx=0):\n",
                "    X, y = [], []\n",
                "    for i in range(len(data) - seq_length):\n",
                "        X.append(data[i:i+seq_length])\n",
                "        y.append(data[i+seq_length, target_idx])\n",
                "    return np.array(X), np.array(y)\n",
                "\n",
                "# Adjust sequence length for available data\n",
                "seq_len = min(SEQUENCE_LENGTH, len(scaled_data) - 2)\n",
                "X, y = create_sequences(scaled_data, seq_len)\n",
                "\n",
                "print(f\"Sequences: X={X.shape}, y={y.shape}\")\n",
                "\n",
                "# Split data\n",
                "n = len(X)\n",
                "train_end = int(n * 0.7)\n",
                "val_end = int(n * 0.85)\n",
                "\n",
                "X_train, y_train = X[:train_end], y[:train_end]\n",
                "X_val, y_val = X[train_end:val_end], y[train_end:val_end]\n",
                "X_test, y_test = X[val_end:], y[val_end:]\n",
                "\n",
                "# Handle empty splits\n",
                "if len(X_val) == 0:\n",
                "    X_val, y_val = X_train, y_train\n",
                "if len(X_test) == 0:\n",
                "    X_test, y_test = X_val, y_val\n",
                "\n",
                "print(f\"Train: {X_train.shape}, Val: {X_val.shape}, Test: {X_test.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 5. Research-Grade Model Architectures"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Temporal Fusion Transformer (TFT)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_tft(seq_length, n_features, d_model=128, num_heads=8, dropout=0.1):\n",
                "    \"\"\"Temporal Fusion Transformer - ~2.5M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    \n",
                "    # Input embedding\n",
                "    x = Dense(d_model)(inputs)\n",
                "    x = LayerNormalization()(x)\n",
                "    \n",
                "    # Bidirectional LSTM encoder\n",
                "    x = Bidirectional(LSTM(d_model, return_sequences=True, dropout=dropout))(x)\n",
                "    x = Bidirectional(LSTM(d_model // 2, return_sequences=True, dropout=dropout))(x)\n",
                "    x = LayerNormalization()(x)\n",
                "    \n",
                "    # Self-attention\n",
                "    attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x, x)\n",
                "    x = LayerNormalization()(x + attn)\n",
                "    \n",
                "    # Gated skip connection\n",
                "    gate = Dense(d_model, activation='sigmoid')(x)\n",
                "    x = x * gate\n",
                "    \n",
                "    # Feed-forward\n",
                "    ff = Dense(d_model * 4, activation='gelu')(x)\n",
                "    ff = Dropout(dropout)(ff)\n",
                "    ff = Dense(d_model)(ff)\n",
                "    x = LayerNormalization()(x + ff)\n",
                "    \n",
                "    # Output\n",
                "    x = GlobalAveragePooling1D()(x)\n",
                "    x = Dense(64, activation='relu')(x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    outputs = Dense(1, dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='TFT')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "tft_model = build_tft(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"TFT Parameters: {tft_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 N-BEATS"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_nbeats(seq_length, n_features, num_stacks=4, num_blocks=4, hidden=512, dropout=0.1):\n",
                "    \"\"\"N-BEATS - Neural Basis Expansion Analysis - ~3M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    x = Flatten()(inputs)\n",
                "    \n",
                "    residual = x\n",
                "    forecasts = []\n",
                "    \n",
                "    for s in range(num_stacks):\n",
                "        for b in range(num_blocks):\n",
                "            h = Dense(hidden, activation='relu')(residual)\n",
                "            h = BatchNormalization()(h)\n",
                "            h = Dropout(dropout)(h)\n",
                "            for _ in range(3):\n",
                "                h = Dense(hidden, activation='relu')(h)\n",
                "                h = Dropout(dropout)(h)\n",
                "            \n",
                "            backcast = Dense(seq_length * n_features)(h)\n",
                "            forecast = Dense(32)(h)\n",
                "            \n",
                "            residual = residual - backcast\n",
                "            forecasts.append(forecast)\n",
                "    \n",
                "    combined = Add()(forecasts) if len(forecasts) > 1 else forecasts[0]\n",
                "    outputs = Dense(1, dtype='float32')(combined)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='NBEATS')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "nbeats_model = build_nbeats(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"N-BEATS Parameters: {nbeats_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.3 Informer"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_informer(seq_length, n_features, d_model=256, num_heads=8, num_layers=4, dropout=0.1):\n",
                "    \"\"\"Informer - Efficient Long-Sequence Transformer - ~4M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    \n",
                "    x = Dense(d_model)(inputs)\n",
                "    x = PositionalEncoding(max_len=seq_length * 2, d_model=d_model)(x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    \n",
                "    for i in range(num_layers):\n",
                "        attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x, x)\n",
                "        x = LayerNormalization()(x + Dropout(dropout)(attn))\n",
                "        \n",
                "        ff = Dense(d_model * 2, activation='gelu')(x)\n",
                "        ff = Dense(d_model)(ff)\n",
                "        x = LayerNormalization()(x + Dropout(dropout)(ff))\n",
                "        \n",
                "        if i < num_layers - 1:\n",
                "            x = Conv1D(d_model, 3, padding='same', activation='elu')(x)\n",
                "    \n",
                "    x = GlobalAveragePooling1D()(x)\n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    x = Dense(64, activation='relu')(x)\n",
                "    outputs = Dense(1, dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='Informer')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "informer_model = build_informer(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"Informer Parameters: {informer_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.4 WaveNet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_wavenet(seq_length, n_features, channels=128, layers=10, stacks=2, dropout=0.1):\n",
                "    \"\"\"WaveNet - Dilated Causal Convolutions - ~2M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    x = Conv1D(channels, 1)(inputs)\n",
                "    \n",
                "    skips = []\n",
                "    for s in range(stacks):\n",
                "        for i in range(layers):\n",
                "            dilation = 2 ** i\n",
                "            tanh_out = Conv1D(channels, 2, padding='causal', dilation_rate=dilation, activation='tanh')(x)\n",
                "            sigmoid_out = Conv1D(channels, 2, padding='causal', dilation_rate=dilation, activation='sigmoid')(x)\n",
                "            gated = tanh_out * sigmoid_out\n",
                "            \n",
                "            skip = Conv1D(channels * 2, 1)(gated)\n",
                "            skips.append(skip)\n",
                "            \n",
                "            residual = Conv1D(channels, 1)(gated)\n",
                "            x = Add()([x, residual])\n",
                "            x = LayerNormalization()(x)\n",
                "    \n",
                "    skip_sum = Add()(skips)\n",
                "    skip_sum = keras.activations.relu(skip_sum)\n",
                "    skip_sum = Conv1D(256, 1, activation='relu')(skip_sum)\n",
                "    \n",
                "    x = GlobalAveragePooling1D()(skip_sum)\n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    outputs = Dense(1, dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='WaveNet')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "wavenet_model = build_wavenet(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"WaveNet Parameters: {wavenet_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.5 DeepAR"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_deepar(seq_length, n_features, lstm_units=(256, 256, 128), dropout=0.2):\n",
                "    \"\"\"DeepAR - Autoregressive RNN - ~2.5M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    \n",
                "    x = Dense(64)(inputs)\n",
                "    x = LayerNormalization()(x)\n",
                "    \n",
                "    for i, units in enumerate(lstm_units):\n",
                "        return_seq = i < len(lstm_units) - 1\n",
                "        x = LSTM(units, return_sequences=return_seq, dropout=dropout)(x)\n",
                "        if return_seq:\n",
                "            x = LayerNormalization()(x)\n",
                "    \n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    x = Dense(64, activation='relu')(x)\n",
                "    outputs = Dense(1, dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='DeepAR')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "deepar_model = build_deepar(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"DeepAR Parameters: {deepar_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.6 TCN (Temporal Convolutional Network)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_tcn(seq_length, n_features, channels=[128, 128, 256, 256, 512], kernel_size=3, dropout=0.1):\n",
                "    \"\"\"TCN - Temporal Convolutional Network - ~3M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    x = inputs\n",
                "    \n",
                "    for i, ch in enumerate(channels):\n",
                "        dilation = 2 ** i\n",
                "        conv1 = Conv1D(ch, kernel_size, padding='causal', dilation_rate=dilation, activation='relu')(x)\n",
                "        conv1 = BatchNormalization()(conv1)\n",
                "        conv1 = Dropout(dropout)(conv1)\n",
                "        \n",
                "        conv2 = Conv1D(ch, kernel_size, padding='causal', dilation_rate=dilation, activation='relu')(conv1)\n",
                "        conv2 = BatchNormalization()(conv2)\n",
                "        conv2 = Dropout(dropout)(conv2)\n",
                "        \n",
                "        if x.shape[-1] != ch:\n",
                "            x = Conv1D(ch, 1)(x)\n",
                "        x = Add()([x, conv2])\n",
                "        x = LayerNormalization()(x)\n",
                "    \n",
                "    avg_pool = GlobalAveragePooling1D()(x)\n",
                "    max_pool = GlobalMaxPooling1D()(x)\n",
                "    x = Concatenate()([avg_pool, max_pool])\n",
                "    \n",
                "    x = Dense(256, activation='relu')(x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    outputs = Dense(1, dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='TCN')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "tcn_model = build_tcn(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"TCN Parameters: {tcn_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.7 Hybrid Attention Network"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def build_hybrid(seq_length, n_features, d_model=256, num_heads=8, num_layers=6, dropout=0.1):\n",
                "    \"\"\"Hybrid Attention Network - CNN + LSTM + Transformer - ~5M parameters.\"\"\"\n",
                "    inputs = Input(shape=(seq_length, n_features))\n",
                "    \n",
                "    # Multi-scale CNN\n",
                "    conv1 = Conv1D(64, 3, padding='same', activation='relu')(inputs)\n",
                "    conv2 = Conv1D(64, 5, padding='same', activation='relu')(inputs)\n",
                "    conv3 = Conv1D(64, 7, padding='same', activation='relu')(inputs)\n",
                "    cnn = Concatenate()([conv1, conv2, conv3])\n",
                "    cnn = Conv1D(d_model, 1)(cnn)\n",
                "    cnn = BatchNormalization()(cnn)\n",
                "    \n",
                "    # Bidirectional LSTM\n",
                "    lstm = Bidirectional(LSTM(d_model // 2, return_sequences=True))(inputs)\n",
                "    lstm = LayerNormalization()(lstm)\n",
                "    \n",
                "    # Combine\n",
                "    x = Add()([cnn, lstm])\n",
                "    x = PositionalEncoding(max_len=seq_length * 2, d_model=d_model)(x)\n",
                "    \n",
                "    # Transformer layers\n",
                "    for _ in range(num_layers):\n",
                "        attn = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x, x)\n",
                "        x = LayerNormalization()(x + Dropout(dropout)(attn))\n",
                "        \n",
                "        ff = Dense(d_model * 4, activation='gelu')(x)\n",
                "        ff = Dense(d_model)(ff)\n",
                "        x = LayerNormalization()(x + Dropout(dropout)(ff))\n",
                "    \n",
                "    # Cross-attention\n",
                "    cross = layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model // num_heads)(x, cnn)\n",
                "    x = LayerNormalization()(x + cross)\n",
                "    \n",
                "    x = GlobalAveragePooling1D()(x)\n",
                "    x = Dense(256, activation='relu')(x)\n",
                "    x = Dropout(dropout)(x)\n",
                "    x = Dense(128, activation='relu')(x)\n",
                "    outputs = Dense(1, dtype='float32')(x)\n",
                "    \n",
                "    model = Model(inputs, outputs, name='HybridAttention')\n",
                "    model.compile(optimizer=keras.optimizers.AdamW(1e-4), loss='huber', metrics=['mae', 'mse'])\n",
                "    return model\n",
                "\n",
                "hybrid_model = build_hybrid(X_train.shape[1], X_train.shape[2])\n",
                "print(f\"Hybrid Parameters: {hybrid_model.count_params():,}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# All models summary\n",
                "models = {\n",
                "    'TFT': tft_model,\n",
                "    'N-BEATS': nbeats_model,\n",
                "    'Informer': informer_model,\n",
                "    'WaveNet': wavenet_model,\n",
                "    'DeepAR': deepar_model,\n",
                "    'TCN': tcn_model,\n",
                "    'Hybrid': hybrid_model\n",
                "}\n",
                "\n",
                "print(\"\\n\" + \"=\"*50)\n",
                "print(\"RESEARCH-GRADE MODEL SUMMARY\")\n",
                "print(\"=\"*50)\n",
                "total = 0\n",
                "for name, model in models.items():\n",
                "    params = model.count_params()\n",
                "    total += params\n",
                "    print(f\"{name}: {params:,} parameters\")\n",
                "print(f\"\\nTOTAL: {total:,} parameters (~{total/1e6:.1f}M)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 6. Training All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def get_callbacks(name):\n",
                "    return [\n",
                "        EarlyStopping(monitor='val_loss', patience=30, restore_best_weights=True),\n",
                "        ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, min_lr=1e-7)\n",
                "    ]"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%%time\n",
                "# Train all models\n",
                "histories = {}\n",
                "batch_size = min(BATCH_SIZE, len(X_train))\n",
                "\n",
                "for name, model in models.items():\n",
                "    print(f\"\\n{'='*50}\")\n",
                "    print(f\"Training {name}\")\n",
                "    print(f\"{'='*50}\")\n",
                "    \n",
                "    histories[name] = model.fit(\n",
                "        X_train, y_train,\n",
                "        validation_data=(X_val, y_val),\n",
                "        epochs=EPOCHS,\n",
                "        batch_size=batch_size,\n",
                "        callbacks=get_callbacks(name),\n",
                "        verbose=1\n",
                "    )"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 7. Evaluation"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def evaluate_model(model, name, X_test, y_test, target_scaler):\n",
                "    y_pred_scaled = model.predict(X_test, verbose=0).flatten()\n",
                "    y_pred = target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).flatten()\n",
                "    y_true = target_scaler.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
                "    \n",
                "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
                "    mae = mean_absolute_error(y_true, y_pred)\n",
                "    mape = np.mean(np.abs((y_true - y_pred) / (y_true + 1e-8))) * 100\n",
                "    r2 = r2_score(y_true, y_pred) if len(y_true) > 1 else 0.0\n",
                "    \n",
                "    return {\n",
                "        'Model': name, 'RMSE': rmse, 'MAE': mae, 'MAPE': mape, 'R2': r2,\n",
                "        'y_pred': y_pred, 'y_true': y_true, 'params': model.count_params()\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all\n",
                "results = []\n",
                "for name, model in models.items():\n",
                "    result = evaluate_model(model, name, X_test, y_test, target_scaler)\n",
                "    results.append(result)\n",
                "    print(f\"{name}: RMSE={result['RMSE']:.2f}, MAE={result['MAE']:.2f}, MAPE={result['MAPE']:.2f}%\")\n",
                "\n",
                "results_df = pd.DataFrame([{k: v for k, v in r.items() if k not in ['y_pred', 'y_true']} for r in results])\n",
                "results_df.sort_values('RMSE')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "\n",
                "# Training loss\n",
                "for name, history in histories.items():\n",
                "    axes[0, 0].plot(history.history['loss'], label=name)\n",
                "axes[0, 0].set_title('Training Loss')\n",
                "axes[0, 0].legend()\n",
                "axes[0, 0].grid(True, alpha=0.3)\n",
                "\n",
                "# Validation loss\n",
                "for name, history in histories.items():\n",
                "    axes[0, 1].plot(history.history['val_loss'], label=name)\n",
                "axes[0, 1].set_title('Validation Loss')\n",
                "axes[0, 1].legend()\n",
                "axes[0, 1].grid(True, alpha=0.3)\n",
                "\n",
                "# RMSE comparison\n",
                "colors = plt.cm.viridis(np.linspace(0, 1, len(results_df)))\n",
                "bars = axes[1, 0].bar(results_df['Model'], results_df['RMSE'], color=colors)\n",
                "axes[1, 0].set_title('RMSE Comparison')\n",
                "axes[1, 0].tick_params(axis='x', rotation=45)\n",
                "\n",
                "# Params vs RMSE\n",
                "axes[1, 1].scatter(results_df['params'] / 1e6, results_df['RMSE'], c=colors, s=100)\n",
                "for i, row in results_df.iterrows():\n",
                "    axes[1, 1].annotate(row['Model'], (row['params'] / 1e6, row['RMSE']), fontsize=8)\n",
                "axes[1, 1].set_xlabel('Parameters (Millions)')\n",
                "axes[1, 1].set_ylabel('RMSE')\n",
                "axes[1, 1].set_title('Parameters vs Performance')\n",
                "axes[1, 1].grid(True, alpha=0.3)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('research_model_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "---\n",
                "## 8. Conclusions\n",
                "\n",
                "### Research Contributions\n",
                "- Implemented **7 state-of-the-art deep learning architectures** for agricultural price prediction\n",
                "- Total of **~20M+ parameters** across all models\n",
                "- Comprehensive comparison of temporal modeling approaches\n",
                "\n",
                "### Model Architectures\n",
                "| Model | Parameters | Description |\n",
                "|-------|------------|-------------|\n",
                "| TFT | ~2.5M | Temporal Fusion Transformer |\n",
                "| N-BEATS | ~3M | Neural Basis Expansion |\n",
                "| Informer | ~4M | Efficient Long-Sequence Transformer |\n",
                "| WaveNet | ~2M | Dilated Causal Convolutions |\n",
                "| DeepAR | ~2.5M | Autoregressive RNN |\n",
                "| TCN | ~3M | Temporal Convolutional Network |\n",
                "| Hybrid | ~5M | CNN + LSTM + Transformer |\n",
                "\n",
                "### Research Paper References\n",
                "1. Lim et al. (2021) - *Temporal Fusion Transformers*\n",
                "2. Oreshkin et al. (2020) - *N-BEATS*\n",
                "3. Zhou et al. (2021) - *Informer*\n",
                "4. Van den Oord et al. (2016) - *WaveNet*\n",
                "5. Salinas et al. (2020) - *DeepAR*\n",
                "6. Bai et al. (2018) - *TCN*"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}