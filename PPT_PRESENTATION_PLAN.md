# üåæ AgriCast: PPT Presentation Plan (20 Slides)

**Course:** Deep Learning with TensorFlow - CSE 3793  
**Git Commit:** `370dea75` - All 10 model architectures with visualizations

---

## Slide 1: Title Slide

| Element | Content |
|---------|---------|
| **Title** | üåæ AgriCast: Agricultural Commodity Price Prediction |
| **Subtitle** | Deep Learning with TensorFlow - CSE 3793 Major Assignment |
| **Key Stats** | 10 Deep Learning Architectures ‚Ä¢ 350M+ Parameters ‚Ä¢ Best RMSE: 634.74 |
| **Visual** | `outputs/figures/comparison/05_radar_chart.png` |

---

## Slide 2: Project Overview & Objectives

| Element | Content |`
|---------|---------|
| **Title** | Project Overview |
| **Content** | |
| | ‚Ä¢ Industry-grade intelligent system for agricultural commodity price prediction |
| | ‚Ä¢ Uses 10 advanced deep learning architectures (LSTM, GRU, Transformer, TCN, WaveNet, N-BEATS, TFT, ConvLSTM, DenseNN, Attention) |
| | ‚Ä¢ 350+ million total trainable parameters |
| | ‚Ä¢ GPU-optimized with mixed precision training on RTX 4060 |

---

## Slide 3: üèÜ Model Performance Leaderboard

| Rank | Model | RMSE ‚Üì | R¬≤ Score | Parameters |
|:----:|:------|-------:|---------:|-----------:|
| ü•á | **TCN** | **634.74** | **0.469** | 29.6M |
| ü•à | WaveNet | 701.54 | 0.351 | 32.1M |
| ü•â | GRU | 710.60 | 0.335 | 68.0M |
| 4 | Attention | 714.24 | 0.328 | 28.6M |
| 5 | Transformer | 721.62 | 0.314 | 50.8M |
| 6 | LSTM | 724.23 | 0.309 | 40.6M |
| 7 | TFT | 770.05 | 0.219 | 14.2M |
| 8 | ConvLSTM | 886.83 | -0.036 | 13.1M |
| 9 | DenseNN | 943.09 | -0.172 | 14.6M |
| 10 | N-BEATS | 1111.84 | -0.629 | 29.6M |

**Figure:** `outputs/figures/comparison/01_metrics_bars.png`

---

## Slide 4: Dataset - Multi-Source Compilation

| Source | Dataset | Records | Coverage |
|--------|---------|--------:|----------|
| data.gov.in | Agriculture Commodities | 23,094 | India - Weekly |
| Kaggle | WFP India Food Prices | ~15,000 | India - UN WFP |
| Kaggle | Vegetables & Fruits | ~8,000 | Nepal - Kalimati |
| Kaggle | WFP Global Food Prices | ~50,000 | Global - 80+ countries |
| Kaggle | Commodity Prices 1960-2021 | ~3,000 | Global - Historical |
| Kaggle | Crop Price Prediction | ~2,000 | India - Crop yields |

**Final Dataset:** 7,015 daily records ‚Ä¢ 32 years (1992-2024) ‚Ä¢ 220 KB

---

## Slide 5: Data Processing & Feature Engineering

**Pipeline:** Raw Data ‚Üí Standardization ‚Üí Cleaning ‚Üí Daily Aggregation ‚Üí Final Dataset

| Feature | Type | Description |
|---------|------|-------------|
| price, log_price | Raw/Transform | Original and log-transformed |
| pct_change | Derived | Day-over-day change |
| ma_7, ma_14, ma_30 | Rolling | Moving averages |
| std_7, std_14, std_30 | Rolling | Standard deviations |
| momentum | Derived | Price deviation from MA_7 |

**Data Split:** Train 70% (4,910) ‚Ä¢ Val 15% (1,052) ‚Ä¢ Test 15% (1,053) ‚Ä¢ Sequence: 60 days

---

## Slide 6: ü•á TCN - Best Performing Model

**RMSE: 634.74 | R¬≤: 0.469 | Params: 29.6M**

```
Input (60√ó10) ‚Üí Conv1D(512) ‚Üí 18√ó Dilated Causal Blocks ‚Üí GAP ‚Üí Dense ‚Üí Output
```

**Key Features:**
- Dilated convolutions capture long-range dependencies
- Causal padding prevents future information leakage
- Residual connections enable very deep networks

**Figures:** `tcn/predictions.png` + `tcn/training_curves.png`

---

## Slide 7: ü•à WaveNet & ü•â GRU

**WaveNet** (RMSE: 701.54 | R¬≤: 0.351 | 32.1M)
```
Input ‚Üí Conv1D ‚Üí 22√ó Gated Dilated Blocks ‚Üí Skip Aggregation ‚Üí Dense ‚Üí Output
```
- Gated activations from audio synthesis
- Skip connections aggregate multi-scale patterns

**GRU** (RMSE: 710.60 | R¬≤: 0.335 | 68.0M)
```
Input ‚Üí Dense(1024) ‚Üí 8√ó BiGRU Residual Blocks ‚Üí Attention(16h) ‚Üí Dense ‚Üí Output
```
- Bidirectional processing captures past/future context
- Deep residual connections prevent vanishing gradients

**Figures:** `wavenet/predictions.png` + `gru/predictions.png`

---

## Slide 8: Attention & Transformer Models

**Attention** (RMSE: 714.24 | R¬≤: 0.328 | 28.6M)
```
Input ‚Üí Embedding(384) ‚Üí Positional Enc ‚Üí 12√ó Pre-Norm Attention Blocks ‚Üí Output
```

**Transformer** (RMSE: 721.62 | R¬≤: 0.314 | 50.8M)
```
Input ‚Üí Embedding(512) ‚Üí Positional Enc ‚Üí 12√ó Pre-Norm Transformer Blocks ‚Üí Output
```

**Key Features:**
- Pre-normalization ensures gradient stability
- Fully parallelizable (no recurrence)
- 16 heads, key_dim=64

**Figures:** `attention/predictions.png` + `transformer/predictions.png`

---

## Slide 9: LSTM & TFT Models

**LSTM** (RMSE: 724.23 | R¬≤: 0.309 | 40.6M)
```
Input ‚Üí Dense(768) ‚Üí 5√ó BiLSTM ‚Üí 2√ó Multi-Head Attention ‚Üí Dense ‚Üí Output
```
- Classic LSTM with dual attention layers

**TFT** (RMSE: 770.05 | R¬≤: 0.219 | 14.2M)
```
Input ‚Üí Variable Selection ‚Üí 3√ó BiLSTM ‚Üí 2√ó Attention ‚Üí Gated Skip ‚Üí Output
```
- Variable selection learns feature importance
- **Most efficient** (best R¬≤/params ratio)

**Figures:** `lstm/predictions.png` + `tft/predictions.png`

---

## Slide 10: ConvLSTM, DenseNN & N-BEATS

**ConvLSTM** (RMSE: 886.83 | Params: 13.1M)
- 6√ó Conv1D ‚Üí MaxPool ‚Üí 4√ó BiLSTM ‚Üí Dense
- CNN extracts local patterns, LSTM captures temporal

**DenseNN** (RMSE: 943.09 | Params: 14.6M)
- Flatten ‚Üí 11√ó Dense Layers (2048‚Üí128) with GELU, Dropout
- Pure MLP baseline, no temporal inductive bias

**N-BEATS** (RMSE: 1111.84 | Params: 29.6M)
- 12√ó N-BEATS Blocks with Backcast/Forecast branches
- Interpretable time series decomposition

**Figures:** `convlstm/predictions.png` + `densenn/predictions.png` + `nbeats/predictions.png`

---

## Slide 11: All Model Predictions vs Actual

**Visual:** `outputs/figures/comparison/03_predictions_overlay.png` (FULL WIDTH)

Overlay comparison of all 10 models against actual price data showing:
- TCN tracks actual values most closely
- N-BEATS shows highest deviation
- Top 6 models cluster together in performance

---

## Slide 12: Performance Scatter (RMSE vs R¬≤)

**Visual:** `outputs/figures/comparison/04_performance_scatter.png`

**Insights:**
- TCN occupies optimal position (low RMSE, high R¬≤)
- WaveNet, GRU, Attention, Transformer, LSTM cluster together
- ConvLSTM, DenseNN, N-BEATS show negative R¬≤ (worse than baseline)

---

## Slide 13: Model Comparison Charts

**Grid Layout (2√ó2):**

| Chart | Figure |
|-------|--------|
| Metrics Bar Comparison | `comparison/01_metrics_bars.png` |
| Parameters & Epochs | `comparison/02_params_epochs.png` |
| Radar Chart | `comparison/05_radar_chart.png` |
| Performance Heatmap | `comparison/06_heatmap.png` |

---

## Slide 14: Error Analysis

**Grid Layout (2√ó2):**

| Chart | Figure |
|-------|--------|
| Error Box Plots | `comparison/07_error_boxplots.png` |
| Learning Curves | `comparison/08_learning_curves.png` |
| Residual Plots | `comparison/09_residual_plots.png` |
| Cumulative Error | `comparison/10_cumulative_error.png` |

---

## Slide 15: Efficiency Analysis

**Visual:** `outputs/figures/comparison/11_efficiency_plot.png`

| Insight | Detail |
|---------|--------|
| Most Efficient | TFT - Best R¬≤ per parameter |
| Least Efficient | GRU - 68M params, moderate R¬≤ |
| Best Overall | TCN - Optimal balance |

---

## Slide 16: GPU Configuration & Optimizations

| Feature | Setting |
|---------|---------|
| Hardware | NVIDIA RTX 4060 (8GB VRAM) |
| CUDA | ‚úÖ Enabled |
| Mixed Precision | ‚úÖ FP16 (2x faster, 50% less memory) |
| XLA JIT | ‚úÖ Enabled |
| Gradient Clipping | clipnorm=1.0 |
| Memory Growth | Dynamic |

**Training Config:** AdamW optimizer ‚Ä¢ Model-specific LR (5e-5 to 1e-4) ‚Ä¢ Early stopping (patience=35) ‚Ä¢ Batch size: 32

---

## Slide 17: Key Technical Features

| Category | Features |
|----------|----------|
| **Training Stability** | Pre-LayerNorm for Transformer/Attention, Gradient clipping, Huber loss |
| **Performance** | Mixed precision (FP16), Early stopping, LR reduction on plateau |
| **Visualization** | 12 comparison charts, Per-model curves, 200 DPI quality |

---

## Slide 18: Evaluation Metrics

| Metric | Formula | Interpretation |
|--------|---------|----------------|
| **RMSE** | ‚àö(Œ£(y-≈∑)¬≤/n) | Penalizes large errors |
| **MAE** | Œ£\|y-≈∑\|/n | Average absolute error |
| **MAPE** | 100√óŒ£\|(y-≈∑)/y\|/n | Scale-independent % |
| **R¬≤** | 1 - SS_res/SS_tot | Explained variance |

---

## Slide 19: Conclusion & Key Findings

| Finding | Detail |
|---------|--------|
| ‚úÖ Best Model | TCN (RMSE: 634.74, R¬≤: 0.469) |
| ‚úÖ Convolutional Wins | TCN, WaveNet outperform RNNs |
| ‚úÖ Pre-Norm Critical | Essential for Transformer stability |
| ‚úÖ Efficiency | TFT achieves most with fewest params |

**Why TCN Wins:**
1. Causal convolutions respect temporal order
2. Dilated layers capture long-range dependencies
3. Fully parallelizable (faster than RNNs)
4. Residual connections enable deep networks

---

## Slide 20: Thank You / Q&A

| Element | Content |
|---------|---------|
| **Title** | Thank You |
| **Subtitle** | Questions & Discussion |
| **Course** | Deep Learning with TensorFlow - CSE 3793 |
| **Visual** | `outputs/figures/comparison/05_radar_chart.png` |

---

## üìä Complete Figure List (42 images)

### Individual Model Figures (30 images)
| Model | Files |
|-------|-------|
| TCN | `tcn/predictions.png`, `tcn/training_curves.png`, `tcn/error_analysis.png` |
| WaveNet | `wavenet/predictions.png`, `wavenet/training_curves.png`, `wavenet/error_analysis.png` |
| GRU | `gru/predictions.png`, `gru/training_curves.png`, `gru/error_analysis.png` |
| Attention | `attention/predictions.png`, `attention/training_curves.png`, `attention/error_analysis.png` |
| Transformer | `transformer/predictions.png`, `transformer/training_curves.png`, `transformer/error_analysis.png` |
| LSTM | `lstm/predictions.png`, `lstm/training_curves.png`, `lstm/error_analysis.png` |
| TFT | `tft/predictions.png`, `tft/training_curves.png`, `tft/error_analysis.png` |
| ConvLSTM | `convlstm/predictions.png`, `convlstm/training_curves.png`, `convlstm/error_analysis.png` |
| DenseNN | `densenn/predictions.png`, `densenn/training_curves.png`, `densenn/error_analysis.png` |
| N-BEATS | `nbeats/predictions.png`, `nbeats/training_curves.png`, `nbeats/error_analysis.png` |

### Comparison Figures (12 images)
| # | File | Suggested Slide |
|:-:|------|-----------------|
| 1 | `01_metrics_bars.png` | Slides 3, 13 |
| 2 | `02_params_epochs.png` | Slide 13 |
| 3 | `03_predictions_overlay.png` | Slide 11 |
| 4 | `04_performance_scatter.png` | Slide 12 |
| 5 | `05_radar_chart.png` | Slides 1, 13, 20 |
| 6 | `06_heatmap.png` | Slide 13 |
| 7 | `07_error_boxplots.png` | Slide 14 |
| 8 | `08_learning_curves.png` | Slide 14 |
| 9 | `09_residual_plots.png` | Slide 14 |
| 10 | `10_cumulative_error.png` | Slide 14 |
| 11 | `11_efficiency_plot.png` | Slide 15 |
| 12 | `12_mae_vs_rmse.png` | Slide 14 |

*All paths relative to `outputs/figures/`*
